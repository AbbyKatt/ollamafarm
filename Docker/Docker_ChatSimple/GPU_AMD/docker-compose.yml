version: '3.8'

services:

  #This gives you a ChatGPT style WebUI -> visit it in your browser at http://localhost:8666
  #It automatically connects to the load balancer and the Ollama services
  ollama-webui:
    image: ollamawebui/ollama-webui
    container_name: simplechatgpu_amd_ollama-webui
    restart: always
    networks:
      - devnet
    ports:
      - 8080:8080
    environment:
      - 'OLLAMA_BASE_URL=http://localhost:11434'
      - 'WEBUI_SECRET_KEY='

  #Workers for Ollama services -> these are the services that actually do the work
  ollamaworker1:
    container_name: simplechatgpu_amd_Worker1
    restart: always
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8g
        reservations:
          devices:
            - driver: rocm
              count: 1
              capabilities:
                - gpu
    environment:
      - ROCR_VISIBLE_DEVICES=all
    networks:
      - devnet

networks:
  devnet:
    driver: bridge

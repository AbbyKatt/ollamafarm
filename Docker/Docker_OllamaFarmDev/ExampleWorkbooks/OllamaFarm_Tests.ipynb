{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell world\n"
     ]
    }
   ],
   "source": [
    "#Exampel workbook\n",
    "print(\"Hell world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ping3\n",
      "  Using cached ping3-4.0.8-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached ping3-4.0.8-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ping3\n",
      "Successfully installed ping3-4.0.8\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached ollama-0.3.3-py3-none-any.whl (10 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx, ollama\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 ollama-0.3.3\n"
     ]
    }
   ],
   "source": [
    "#Install some stuff we need\n",
    "\n",
    "!pip install ping3\n",
    "!pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping 'ollamafarm-nginx-proxy' ... 0ms\n",
      "ping 'ollamafarm-nginx-proxy' ... 0ms\n",
      "ping 'ollamafarm-nginx-proxy' ... 0ms\n",
      "ping 'ollamafarm-nginx-proxy' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker01' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker01' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker01' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker01' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker02' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker02' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker02' ... 0ms\n",
      "ping 'ollamafarm_gpu_worker02' ... 0ms\n"
     ]
    }
   ],
   "source": [
    "from ping3 import ping, verbose_ping\n",
    "verbose_ping('ollamafarm-nginx-proxy', count=4)\n",
    "verbose_ping('ollamafarm_gpu_worker01', count=4)\n",
    "verbose_ping('ollamafarm_gpu_worker02', count=4)\n",
    "#verbose_ping('OllamaDev3', count=4)\n",
    "#verbose_ping('OllamaDev4', count=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Provision environments\n",
    "import ollama\n",
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://ollamafarm_gpu_worker01:11434')\n",
    "client.pull(\"llama3.1\")\n",
    "\n",
    "client = Client(host='http://ollamafarm_gpu_worker02:11434')\n",
    "client.pull(\"llama3.1\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a song about Marmite:\n",
      "\n",
      "**Verse 1**\n",
      "In the land of yeast and spices fine\n",
      "There's a spread that's loved, yet hated in its prime\n",
      "It's dark and it's thick, with a flavor so bold\n",
      "Marmite, oh Marmite, you're a love to be told\n",
      "\n",
      "**Chorus**\n",
      "You're the blackest of spreads, but we adore\n",
      "The way you make our toast, and sometimes more\n",
      "You're the taste of umami, in every single bite\n",
      "Marmite, oh Marmite, we can't live without your delight\n",
      "\n",
      "**Verse 2**\n",
      "Some say you're too strong, that your flavor's too bright\n",
      "But for us, you're a treasure, a culinary sight\n",
      "On crackers or on bread, you're the perfect pair\n",
      "A sprinkle of cheese, and you'll show us you care\n",
      "\n",
      "**Chorus**\n",
      "You're the blackest of spreads, but we adore\n",
      "The way you make our toast, and sometimes more\n",
      "You're the taste of umami, in every single bite\n",
      "Marmite, oh Marmite, we can't live without your delight\n",
      "\n",
      "**Bridge**\n",
      "In the UK, you're a national pride\n",
      "A symbol of tradition, that won't be denied\n",
      "So here's to you, dear Marmite, our love for you will grow\n",
      "In the world of spreads, you'll forever be the best we know!\n",
      "\n",
      "**Chorus**\n",
      "You're the blackest of spreads, but we adore\n",
      "The way you make our toast, and sometimes more\n",
      "You're the taste of umami, in every single bite\n",
      "Marmite, oh Marmite, we can't live without your delight\n",
      "\n",
      "I hope you enjoy this Marmite-themed song!"
     ]
    }
   ],
   "source": [
    "#Streaming test with the Load Balancer!\n",
    "import ollama\n",
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://nginx-proxy:8080')\n",
    "#client.pull(\"llama3.1\")\n",
    "\n",
    "stream = client.chat(\n",
    "    model='llama3.1',\n",
    "    #messages=[{'role': 'user', 'content': 'Why cant fish fly? One line only as a joke'}],\n",
    "    messages=[{'role': 'user', 'content': 'Write me a song abour marmite'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
